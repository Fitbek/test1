

# План ETL
# 1) Проверить доступность API и данных
# 2) Загрузить данные(в локальную файловую систему, на сервер аирфлоу)
# 3) Переместить эти данные в HDFS. Полсе нужно почистить за собой файл/ы  на сервере Airflow
# 4) Обработать данные - Spark Job, который будет агрегировать данные на уровень стран и считать статистики.
# 5) Создать Hive таблицу поверх обработанных данных.
#   А. Проверить, естли нужная нам таблица в Hive.
#   B.Если таблица есть, то пропустить создание этой таблицы и перейти сразу к   обновлению списка партиций.
#   C. Если таблица отсутствует, то нам нужно ее создать.
# 6) Нужно обновить список партиций в таблие.
